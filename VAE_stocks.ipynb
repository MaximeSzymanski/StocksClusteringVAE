{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MaximeSzymanski/StocksClusteringVAE/blob/main/VAE_stocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "JWeowTDmufGn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "!pip install yfinance\n",
    "!pip install BeautifulSoup\n",
    "!pip install requests\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhNDk_sdufGo",
    "outputId": "95a697a2-022b-42e4-aa11-e5368978c61d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def download_stocks(ticker_list, start_date='2019-11-08', end_date='2023-11-08'):\n",
    "    # Set seed for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Initialize an empty list to store normalized values\n",
    "    normalized_values_list = []\n",
    "    stocks_kept = []\n",
    "    iterator = 0\n",
    "    kept_stocks_with_indexes = []\n",
    "\n",
    "    # Loop through each stock symbol\n",
    "    for index, symbol in enumerate(ticker_list):\n",
    "        try:\n",
    "            # Download historical stock data using yfinance\n",
    "            stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "\n",
    "            # Extract the 'Close' column\n",
    "            close_values = stock_data['Close'].values\n",
    "            if len(close_values) >= 0:\n",
    "                # Normalize the data between -1 and 1\n",
    "                normalized_values = 2 * (close_values - np.min(close_values)) / np.ptp(close_values) - 1\n",
    "                normalized_values_list.append(normalized_values)\n",
    "                stocks_kept.append(symbol)\n",
    "                print(f\"stocks {iterator}/{len(ticker_list)}\")\n",
    "                iterator += 1\n",
    "                kept_stocks_with_indexes.append((index, symbol, normalized_values))\n",
    "            else:\n",
    "                # remove the stock if it has not enough data\n",
    "                print(f\"stock {symbol} has not enough data\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading data for {symbol}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # get the different lengths of the stocks\n",
    "    lengths = [len(stock[2]) for stock in kept_stocks_with_indexes]\n",
    "    lengths_set = set(lengths)\n",
    "\n",
    "    # get the most common length\n",
    "    most_common_length = max(set(lengths), key=lengths.count)\n",
    "\n",
    "    # keep only the stocks with the most common length\n",
    "    normalized_values_list = [stock[2] for stock in kept_stocks_with_indexes if len(stock[2]) == most_common_length]\n",
    "    kept_stocks_with_indexes = [(index, symbol) for index, symbol, normalized_values in kept_stocks_with_indexes if len(normalized_values) == most_common_length]\n",
    "\n",
    "    # length is 1006, so we need to remove the last 6 values for each stock\n",
    "    normalized_values_list = [normalized_values[:-6] for normalized_values in normalized_values_list]\n",
    "\n",
    "    return normalized_values_list, kept_stocks_with_indexes"
   ],
   "metadata": {
    "id": "ixrPIZEgufGo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "# request this url https://en.wikipedia.org/wiki/List_of_S%26P_600_companies\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_600_companies'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# get the table by the id constituents\n",
    "table = soup.find('table', {'id': 'constituents'})\n",
    "ticker_list = []\n",
    "# iterate over the tr balise in the table\n",
    "for tr in table.find_all('tr'):\n",
    "    # get the first td balise in the tr balise\n",
    "    td = tr.findAll('td')\n",
    "    # if the td balise is not empty\n",
    "    if td is not None and len(td) > 1:\n",
    "        # get the first a balise in the td balise\n",
    "\n",
    "        a = td[0].find('a')\n",
    "        # if the a balise is not empty\n",
    "        if a is not None:\n",
    "            # get the text of the a balise\n",
    "            ticker = a.text\n",
    "            # add the ticker to the list\n",
    "            print(f\"ticker : {ticker}\")\n",
    "            ticker_list.append(ticker)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists('stocks_data.pkl'):\n",
    "    # If the file exists, load it\n",
    "    with open('stocks_data.pkl', 'rb') as f:\n",
    "        stocks_data = pickle.load(f)\n",
    "else:\n",
    "    # If the file doesn't exist, download the stocks data\n",
    "    # This is a placeholder for the code to download the stocks data\n",
    "    # Replace it with the actual code to download the data\n",
    "    stocks_data = download_stocks(ticker_list)\n",
    "\n",
    "    # Save the downloaded data into a file\n",
    "    with open('stocks_data.pkl', 'wb') as f:\n",
    "        pickle.dump(stocks_data, f)\n",
    "\n",
    "kept_stocks_with_indexes = stocks_data[1]\n",
    "normalized_values_list = stocks_data[0]\n",
    "print(f\"kept_stocks_with_indexes : {kept_stocks_with_indexes}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Use the previously prepared data\n",
    "sub_series_length = 200\n",
    "stock_symbols = [stock[1] for stock in kept_stocks_with_indexes]\n",
    "\n",
    "# Check that the length of each normalized series is divisible by sub_series_length\n",
    "number_of_sub_series = len(normalized_values_list[0]) // sub_series_length\n",
    "\n",
    "for i in range(len(stock_symbols)):\n",
    "    assert len(normalized_values_list[i]) % sub_series_length == 0\n",
    "\n",
    "# Create sub-series for each stock\n",
    "sub_series_list = [normalized_values.reshape((-1, sub_series_length)) for normalized_values in normalized_values_list]\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "stocks_np_array = np.zeros((len(stock_symbols), sub_series_list[0].shape[0], sub_series_list[0].shape[1]))\n",
    "for i in range(len(stock_symbols)):\n",
    "    stocks_np_array[i] = sub_series_list[i]\n",
    "\n",
    "# Create a PyTorch Dataset\n",
    "stock_dataset = StockDataset(stocks_np_array)\n",
    "\n",
    "# Create a PyTorch DataLoader\n",
    "batch_size = 32\n",
    "shuffle = True  # To shuffle the data\n",
    "data_loader = DataLoader(stock_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Iterate through the DataLoader to get shuffled batches\n",
    "for batch in data_loader:\n",
    "    # Process each batch\n",
    "    print(\"Batch shape:\", batch.shape)  # Adjust this according to your processing needs\n",
    "    # Your processing logic here\n"
   ],
   "metadata": {
    "id": "VGQJ_f9PufGo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print a random sub series\n",
    "number_of_stocks = len(stock_symbols)\n",
    "random_index_stocks = np.random.randint(0, number_of_stocks)\n",
    "random_index_sub_series = np.random.randint(0, number_of_sub_series)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(stocks_np_array[random_index_stocks][random_index_sub_series])\n",
    "plt.title('Random Sub Series')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "ppp0apqNufGo",
    "outputId": "29cff1cd-21fa-4c5b-c969-3f9691b26500"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SamplingLayerVAE(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SamplingLayerVAE, self).__init__()\n",
    "\n",
    "  def forward(self, mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, seq_len, n_features, embedding_dim=64,num_layers=3,bidirectional=False,\n",
    "               transfromer_hidden_size_attention = 128,num_heads=8,sub_seq_len=20,\n",
    "               latent_dim=64):\n",
    "    super(Encoder, self).__init__()\n",
    "    assert seq_len % sub_seq_len  == 0\n",
    "    self.number_of_sub_seq = seq_len // sub_seq_len\n",
    "    self.sub_seq_len = sub_seq_len\n",
    "    self.seq_len, self.n_features = seq_len, n_features\n",
    "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "    self.num_layers = num_layers\n",
    "    self.bidirectional = bidirectional\n",
    "    self.chunk_index_embedding = ChunkIndexEmbedding(num_chunks=self.seq_len,embedding_dim=embedding_dim)\n",
    "\n",
    "    self.mu = nn.Linear((self.sub_seq_len//2)*self.embedding_dim, latent_dim)\n",
    "    self.log_var = nn.Linear((self.sub_seq_len//2)*self.embedding_dim, latent_dim)\n",
    "    self.sampling_layer = SamplingLayerVAE()\n",
    "    self.rnn1 = nn.LSTM(\n",
    "          input_size=embedding_dim,\n",
    "          hidden_size=embedding_dim,\n",
    "          num_layers=self.num_layers,\n",
    "          batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "\n",
    "    multiple_bi = 2 if bidirectional else 1\n",
    "\n",
    "\n",
    "    #self.positional_encoding = PositionalEncoding(embedding_dim)\n",
    "    self.embedding_layer = nn.Linear(n_features, embedding_dim)\n",
    "    self.transformer_1 = CompressWithAttentionResidual(input_size=self.hidden_dim,output_size=embedding_dim,\n",
    "                                             hidden_size_attention=embedding_dim*2,\n",
    "                                             group_size=self.number_of_sub_seq)\n",
    "\n",
    "    self.transformer_2 = CompressWithAttentionResidual(input_size=embedding_dim,output_size=embedding_dim,\n",
    "                                             hidden_size_attention=embedding_dim,\n",
    "                                             group_size=2)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, seq_len, n_features = x.size()\n",
    "    original_x = x\n",
    "\n",
    "    #x = x.reshape((batch_size * seq_len, n_features))\n",
    "    x = self.embedding_layer(x)\n",
    "\n",
    "    x = self.chunk_index_embedding(x)\n",
    "    x, (_, _) = self.rnn1(x)\n",
    "\n",
    "\n",
    "    x = self.transformer_1(x)\n",
    "    x = self.transformer_2(x)\n",
    "    x = x.reshape((batch_size, (self.sub_seq_len//2)*self.embedding_dim))\n",
    "    mu = self.mu(x)\n",
    "    log_var = self.log_var(x)\n",
    "    z = self.sampling_layer(mu, log_var)\n",
    "\n",
    "    return z, mu, log_var, original_x\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, seq_len, input_dim=64, n_features=1,num_layers=3,bidirectional=False,\n",
    "               sub_seq_len=20,latent_dim=64):\n",
    "    super(Decoder, self).__init__()\n",
    "    assert seq_len % sub_seq_len  == 0\n",
    "    self.number_of_sub_seq = seq_len // sub_seq_len\n",
    "    # number\n",
    "    self.sub_seq_len = sub_seq_len\n",
    "    self.seq_len, self.input_dim = seq_len, input_dim\n",
    "    self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
    "    self.num_layers = num_layers\n",
    "    self.bidirectional = bidirectional\n",
    "    self.expend_layer_1 = ExpandWithAttentionResidual(input_size=latent_dim,output_size=input_dim,\n",
    "                                             hidden_size_attention=latent_dim,\n",
    "                                             group_size=self.sub_seq_len ,expansion_factor=\n",
    "                                                    self.number_of_sub_seq)\n",
    "\n",
    "    self.expend_layer_2 = ExpandWithAttentionResidual(input_size=input_dim,output_size=input_dim,\n",
    "                                             hidden_size_attention=input_dim,\n",
    "                                             group_size=self.sub_seq_len*self.number_of_sub_seq,expansion_factor=2)\n",
    "\n",
    "    self.output_embeding = OutputEmbedding(input_dim=self.hidden_dim,seq_len=self.seq_len,output_dim=1)\n",
    "    self.chunk_index_embedding = ChunkIndexEmbedding(num_chunks=self.seq_len,embedding_dim=input_dim)\n",
    "    self.rnn1 = nn.LSTM(\n",
    "          input_size=input_dim,\n",
    "          hidden_size=input_dim,\n",
    "          num_layers=self.num_layers,\n",
    "          batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "    multiple_bi = 2 if bidirectional else 1\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    # x is shape (batch_size, 1, n_features)\n",
    "    # repeat the last dimension to have (batch_size, seq_len, n_features)\n",
    "    x = x.unsqueeze(1)\n",
    "    x = x.repeat(1, self.sub_seq_len//2, 1)\n",
    "\n",
    "    x = self.expend_layer_1(x)\n",
    "    x = self.chunk_index_embedding(x)\n",
    "    x = self.expend_layer_2(x)\n",
    "\n",
    "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "    x = x.reshape((batch_size,self.seq_len, self.hidden_dim))\n",
    "    x = self.output_embeding(x)\n",
    "\n",
    "    # keep only the last layer\n",
    "\n",
    "\n",
    "    return x\n",
    "class ExpandWithAttentionResidual(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size_attention, group_size, expansion_factor):\n",
    "        super(ExpandWithAttentionResidual, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size_attention\n",
    "        self.group_size = group_size\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        # Linear projections for attention\n",
    "        self.Q_linear = nn.Linear(self.input_size, self.expansion_factor * self.hidden_size)\n",
    "        self.K_linear = nn.Linear(self.input_size, self.expansion_factor * self.hidden_size)\n",
    "        self.V_linear = nn.Linear(self.input_size, self.expansion_factor * self.hidden_size)\n",
    "\n",
    "        # Final linear transformations\n",
    "        self.attention_output_linear = nn.Linear(self.expansion_factor * self.hidden_size,  self.expansion_factor * self.hidden_size)\n",
    "        self.output_linear = nn.Linear(self.hidden_size , self.output_size)\n",
    "        self.norma = nn.LayerNorm(self.hidden_size)\n",
    "    def forward(self, x):\n",
    "        # x is shape (batch_size, seq_len, n_features)\n",
    "        batch_size, seq_len, n_features = x.size()\n",
    "        # Use attention to expand the input\n",
    "\n",
    "        x = x.view(batch_size* seq_len , n_features)\n",
    "        Q = self.Q_linear(x)\n",
    "        K = self.K_linear(x)\n",
    "        V = self.V_linear(x)\n",
    "        Q = Q.reshape((batch_size, seq_len, self.expansion_factor * self.hidden_size))\n",
    "        K = K.reshape((batch_size, seq_len, self.expansion_factor * self.hidden_size))\n",
    "        V = V.reshape((batch_size, seq_len, self.expansion_factor * self.hidden_size))\n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / (self.hidden_size ** 0.5)\n",
    "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        attended_values = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Apply final linear transformations\n",
    "        attended_values = attended_values.reshape((batch_size* seq_len , self.expansion_factor * self.hidden_size))\n",
    "        attention_output = self.attention_output_linear(attended_values)\n",
    "\n",
    "        # Reshape attention output into groups of size self.group_size\n",
    "        attention_output = attention_output.view(batch_size, seq_len* self.expansion_factor ,self.hidden_size)\n",
    "\n",
    "        # Add residual connection\n",
    "        # repeat the tensor x to have the same shape as attention_output\n",
    "        # repeat to have from (batch_size, seq_len, n_features) to (batch_size, seq_len* self.expansion_factor ,n_features)\n",
    "        x = x.view(batch_size, seq_len, n_features)\n",
    "        x = x.repeat(1, self.expansion_factor, 1)\n",
    "        out = attention_output + x\n",
    "        batch_size, expended_seq_len, n_features = out.size()\n",
    "        # normalize the output\n",
    "        out = out.reshape((batch_size* expended_seq_len , n_features))\n",
    "        #out = self.norma(out)\n",
    "\n",
    "        # Final linear transformation\n",
    "        out = self.output_linear(out)\n",
    "\n",
    "        # RElu\n",
    "        out = F.relu(out)\n",
    "        out = out.reshape((batch_size, expended_seq_len, self.output_size))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class CompressWithAttentionResidual(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size_attention, group_size):\n",
    "        super(CompressWithAttentionResidual, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        print(f\"ouput size : {output_size}\")\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size_attention\n",
    "        self.group_size = group_size\n",
    "\n",
    "        # Linear projections for attention\n",
    "        self.Q_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.K_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.V_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "\n",
    "        # Final linear transformations\n",
    "        self.attention_output_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.output_linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is shape (batch_size, seq_len, n_features)\n",
    "        batch_size, seq_len, n_features = x.size()\n",
    "        # Reshape input into groups of size self.group_size\n",
    "\n",
    "        x = x.view(batch_size, -1, self.group_size, n_features)\n",
    "        group_len = x.size(1)\n",
    "\n",
    "        # Linear projections for attention\n",
    "        Q = self.Q_linear(x)\n",
    "        K = self.K_linear(x)\n",
    "        V = self.V_linear(x)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / (self.hidden_size ** 0.5)\n",
    "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        attended_values = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Sum along the group dimension\n",
    "        attended_values = attended_values.sum(dim=2)\n",
    "\n",
    "        # Apply final linear transformations\n",
    "        attention_output = self.attention_output_linear(attended_values)\n",
    "\n",
    "\n",
    "        # Add residual connection\n",
    "        # so, we need to pack the tensor x to have the same shape as attention_output. We have to do : x[i] = x[i] + x[i+1] and so on paired by 2\n",
    "        group_size = self.group_size\n",
    "\n",
    "# Create slices and sum them based on the group size\n",
    "        slices = [x[:, i::group_size, :] for i in range(group_size)]\n",
    "\n",
    "        x = sum(slices)\n",
    "        # Final linear transformation\n",
    "\n",
    "        batch_size_2, sub_series_length, n_features = attention_output.size()\n",
    "\n",
    "\n",
    "        x = x.reshape((batch_size_2 , sub_series_length, n_features))\n",
    "        out = attention_output + x\n",
    "\n",
    "        # Final linear transformation\n",
    "        out = self.output_linear(out)\n",
    "\n",
    "        # RElu\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ChunkIndexEmbedding(nn.Module):\n",
    "    def __init__(self,num_chunks=100,embedding_dim=64):\n",
    "        super(ChunkIndexEmbedding, self).__init__()\n",
    "        self.num_chunks = num_chunks\n",
    "        self.embedding = nn.Embedding(self.num_chunks,embedding_dim )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_features = x.size()\n",
    "        chunk_index = torch.arange(0, seq_len, 1).to(x.device)\n",
    "\n",
    "        chunk_index = chunk_index.unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "        chunk_index = self.embedding(chunk_index)\n",
    "\n",
    "        x = x + chunk_index\n",
    "        return x\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self,seq_len, input_dim=1,output_dim=64):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Linear(self.input_dim, self.output_dim)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_features = x.size()\n",
    "        x = x.reshape((batch_size * seq_len, n_features))\n",
    "        x = self.embedding(x)\n",
    "        x = x.reshape((batch_size, seq_len, self.output_dim))\n",
    "        return x\n",
    "\n",
    "class OutputEmbedding(nn.Module):\n",
    "    def __init__(self,seq_len, input_dim=64,output_dim=1):\n",
    "        super(OutputEmbedding, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Linear(self.input_dim, self.output_dim)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_features = x.size()\n",
    "        x = x.reshape((batch_size * seq_len, n_features))\n",
    "        x = self.embedding(x)\n",
    "        x = x.reshape((batch_size, seq_len, self.output_dim))\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "YROjk-wXufGo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mSimpleEncoderLSTM\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      2\u001B[0m   \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, seq_len, n_features, embedding_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m,num_layers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,bidirectional\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28msuper\u001B[39m(SimpleEncoderLSTM, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class SimpleEncoderLSTM(nn.Module):\n",
    "  def __init__(self, seq_len, n_features, embedding_dim=64,num_layers=3,bidirectional=False):\n",
    "    super(SimpleEncoderLSTM, self).__init__()\n",
    "    self.seq_len, self.n_features = seq_len, n_features\n",
    "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "    print(f\"hidden_dim: {self.hidden_dim}\")\n",
    "    self.num_layers = num_layers\n",
    "    self.bidirectional = bidirectional\n",
    "    self.rnn1 = nn.LSTM(\n",
    "          input_size=n_features,\n",
    "          hidden_size=self.hidden_dim,\n",
    "          num_layers=self.num_layers,\n",
    "          batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "\n",
    "    multiple_bi = 2 if bidirectional else 1\n",
    "\n",
    "    self.rnn2 = nn.LSTM(\n",
    "          input_size=multiple_bi * self.hidden_dim,\n",
    "          hidden_size=embedding_dim,\n",
    "          num_layers=self.num_layers,\n",
    "          batch_first=True\n",
    "    )\n",
    "\n",
    "    self.mu = torch.nn.Linear(self.seq_len * embedding_dim, latent_dim)\n",
    "    self.log_var = torch.nn.Linear(self.seq_len * embedding_dim, latent_dim)\n",
    "    self.sampling_layer = SamplingLayerVAE()\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    original_x = x\n",
    "    batch_size, seq_len, n_features = x.size()\n",
    "    #x = x.reshape((batch_size * seq_len, n_features))\n",
    "    x, (_, _) = self.rnn1(x)\n",
    "    x, (hidden_n, _) = self.rnn2(x)\n",
    "    x = x.reshape((batch_size, self.seq_len, self.embedding_dim))\n",
    "    x = x.reshape((batch_size, self.seq_len * self.embedding_dim))\n",
    "    mu = self.mu(x)\n",
    "    log_var = self.log_var(x)\n",
    "    z = self.sampling_layer(mu, log_var)\n",
    "\n",
    "    return z, mu, log_var, x, original_x\n",
    "\n",
    "class SimpleDecoderLSTM(nn.Module):\n",
    "  def __init__(self, seq_len, input_dim=64, n_features=1,num_layers=3,bidirectional=False):\n",
    "    super(SimpleDecoderLSTM, self).__init__()\n",
    "    self.seq_len, self.input_dim = seq_len, input_dim\n",
    "    self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
    "    self.num_layers = num_layers\n",
    "    self.bidirectional = bidirectional\n",
    "    self.rnn1 = nn.LSTM(\n",
    "          input_size=input_dim,\n",
    "          hidden_size=input_dim,\n",
    "          num_layers=self.num_layers,\n",
    "          batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "    multiple_bi = 2 if bidirectional else 1\n",
    "    self.rnn2 = nn.LSTM(\n",
    "      input_size=input_dim * multiple_bi,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=self.num_layers,\n",
    "      batch_first=True\n",
    "    )\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    # x is shape (batch_size, 1, n_features)\n",
    "    # repeat the last dimension to have (batch_size, seq_len, n_features)\n",
    "    x = x.repeat(1, self.seq_len, 1)\n",
    "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
    "    x = x.reshape((batch_size,self.seq_len, self.hidden_dim))\n",
    "    # keep only the last layer\n",
    "\n",
    "\n",
    "    return self.output_layer(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:22:16.232238Z",
     "start_time": "2023-11-15T16:22:16.211463Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RecurrentAutoencoder_LSTM(nn.Module):\n",
    "  def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "    super(RecurrentAutoencoder_LSTM, self).__init__()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.encoder = SimpleEncoderLSTM(seq_len, n_features, embedding_dim,bidirectional=True,num_layers=1).to(device)\n",
    "    self.decoder = SimpleDecoderLSTM(seq_len, embedding_dim, n_features,bidirectional=True,num_layers=1).to(device)\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = self.decoder(x)\n",
    "    x = nn.Tanh()(x)\n",
    "    return x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = RecurrentAutoencoder_LSTM(sub_series_length, 1, 64)\n",
    "print(f\"model : {model}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RecurrentAutoencoder(nn.Module):\n",
    "  def __init__(self, seq_len, n_features, embedding_dim=128,sub_seq_len=10,latent_dim=2):\n",
    "    super(RecurrentAutoencoder, self).__init__()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.encoder = Encoder(seq_len, n_features, embedding_dim,bidirectional=True,num_layers=1,\n",
    "        sub_seq_len=sub_seq_len,latent_dim=latent_dim).to(device)\n",
    "    self.decoder = Decoder(seq_len, embedding_dim, n_features,bidirectional=True,num_layers=1,\n",
    "        sub_seq_len=sub_seq_len,latent_dim=latent_dim).to(device)\n",
    "  def forward(self, x):\n",
    "    z, mu, sigma, original_x = self.encoder(x)\n",
    "    x = self.decoder(z)\n",
    "    x = nn.Tanh()(x)\n",
    "    return x, original_x, mu, sigma"
   ],
   "metadata": {
    "id": "RNCtBAqRufGp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"sub_series_length : {sub_series_length}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_dim = 32\n",
    "model = RecurrentAutoencoder(sub_series_length, 1, 32,sub_seq_len=100,latent_dim=latent_dim).to(device)"
   ],
   "metadata": {
    "id": "j7_jlbF8ufGp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(model)"
   ],
   "metadata": {
    "id": "7bFosZ2dsfvH",
    "outputId": "e844ca22-82b6-44c9-d11a-28324618ca9c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def KL_loss(mu, sigma):\n",
    "    return -0.5 * torch.sum(1 + sigma - mu.pow(2) - sigma.exp())\n",
    "\n",
    "def reconstruction_loss(x, y):\n",
    "\n",
    "    return nn.MSELoss(reduction='sum')(x,y)\n",
    "\n",
    "def loss_function(x, original_x, mu, sigma,k1=1,k2=1e-1):\n",
    "    return k1 *reconstruction_loss(original_x, x) + k2 * KL_loss(mu, sigma)\n"
   ],
   "metadata": {
    "id": "xlV0gPUvufGp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert the numpy array to a PyTorch tensor\n",
    "# The shape is : (number_of_stocks, number_of_sub_series, sub_series_length, features)\n",
    "# We need to reshape it to (number_of_stocks * number_of_sub_series, sub_series_length, features)\n",
    "number_of_stocks = stocks_np_array.shape[0]\n",
    "\n",
    "\n",
    "stocks_np_array_test = stocks_np_array.reshape((number_of_stocks*number_of_sub_series, sub_series_length, 1))\n",
    "signal_to_tensor = torch.from_numpy(stocks_np_array_test).float().to(device)\n",
    "\n",
    "# Forward pass through the model\n",
    "x, original_x, mu, sigma = model(signal_to_tensor)\n",
    "\n",
    "# Reconstruct the good output shape\n",
    "x = x.reshape((number_of_stocks, number_of_sub_series, sub_series_length, 1))\n"
   ],
   "metadata": {
    "id": "RYfQ0EHGufGp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load from cuda to cpu\n",
    "#model = torch.load(\"model.pth\",map_location=torch.device('cpu'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the reconstruction\n",
    "# Plot 10 random stocks\n",
    "number_of_stocks_to_plot = 5\n",
    "random_stocks = np.random.randint(0, number_of_stocks, number_of_stocks_to_plot)\n",
    "print(f\"random stocks : {random_stocks}\")\n",
    "# Plot 10 random sub-series\n",
    "random_sub_series = np.random.randint(0, number_of_sub_series, number_of_stocks_to_plot)\n",
    "print(f\"random sub series : {random_sub_series}\")\n",
    "# Plot the original and reconstructed signals\n",
    "for i in range(number_of_stocks_to_plot):\n",
    "    plt.plot(original_x[random_stocks[i]][random_sub_series[i]].cpu().detach().numpy(), label='Original signal')\n",
    "    plt.plot(x[random_stocks[i]][random_sub_series[i]].cpu().detach().numpy(), label='Reconstructed signal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "id": "IdYqaz71ufGp",
    "outputId": "8f1a825f-69a5-422e-9956-d4c4cd2a060c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# try a simple overfitting\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "batch_size = 32\n",
    "shuffle = True  # To shuffle the data\n",
    "data_loader = DataLoader(stocks_np_array, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "#loss_fn = nn.L1Loss(reduction='sum')\n",
    "\n",
    "for epoch in range(5000):\n",
    "    for batch in data_loader:\n",
    "        # Process each batch\n",
    "        #print(\"Batch shape:\", batch.shape)  # Adjust this according to your processing needs\n",
    "        batch = batch.to(device)\n",
    "        batch = batch.to(torch.float32)\n",
    "        batch = batch.reshape((batch_size*number_of_sub_series, sub_series_length, 1))\n",
    "        # Your processing logic here\n",
    "        print(batch.shape)\n",
    "        optimizer.zero_grad()\n",
    "        x, original_x, mu, sigma = model(batch)\n",
    "        loss = loss_function(x, original_x, mu, sigma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch}, loss: {loss.item()}')"
   ],
   "metadata": {
    "id": "1iyceGB3ufGp",
    "outputId": "e3d28737-130b-4249-d8af-05d07155d36b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "with torch.no_grad():\n",
    "    x, original_x, mu, sigma = model(signal_to_tensor)\n",
    "    x = x.cpu().detach().numpy()\n",
    "print(f\"x shape : {x.shape}\")\n",
    "# Plot the reconstruction of all signals\n",
    "stocks_kept= [stock[1] for stock in kept_stocks_with_indexes]\n",
    "stocks_to_plot = 10\n",
    "# Reshape the data\n",
    "print(f\"number_of_stocks : {number_of_stocks}\")\n",
    "print(f\"number_of_sub_series : {number_of_sub_series}\")\n",
    "print(f\"sub_series_length : {sub_series_length}\")\n",
    "stocks_np_array_reshaped = stocks_np_array.reshape((number_of_stocks, number_of_sub_series, sub_series_length))\n",
    "reconstructed_signals_reshaped = x.reshape((number_of_stocks, number_of_sub_series, sub_series_length, 1))\n",
    "print(f\"stocks_np_array_reshaped shape : {stocks_np_array_reshaped.shape}\")\n",
    "# Plot each stock in separate subplots\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for i in range(stocks_to_plot):\n",
    "    plt.subplot(stocks_to_plot, 1, i + 1)\n",
    "    plt.plot(stocks_np_array_reshaped[i].reshape((-1, 1)), label='Original Signal')\n",
    "    plt.plot(reconstructed_signals_reshaped[i].reshape((-1, 1)), label='Reconstructed Signal')\n",
    "    plt.title(f'Stock {stocks_kept[i]} - Original vs Reconstructed Signals')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "o0mH5xOgufGp",
    "outputId": "f060eed8-981c-471f-f93c-024d23d2a128"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply PCA to the latent space\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# Set seeds for reproducibility\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    signals_latent,_,_,_ = model.encoder(signal_to_tensor)\n",
    "    signals_latent = signals_latent.squeeze().cpu().detach().numpy()\n",
    "print(f\"signals_latent shape : {signals_latent.shape}\")\n",
    "pca = PCA(n_components=2)\n",
    "latent_space = pca.fit_transform(signals_latent)\n",
    "# Reshape the latent space for each stock\n",
    "\n",
    "latent_space_np_array = np.array(latent_space).reshape((number_of_stocks, number_of_sub_series, 2))\n",
    "print(f\"latent_space_np_array shape : {latent_space_np_array.shape}\")\n",
    "# Plot the latent space for each stock\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, latent_space_stock in enumerate(latent_space_np_array):\n",
    "    plt.scatter(latent_space_stock[:, 0], latent_space_stock[:, 1], label=f'{stock_symbols[i]}')\n",
    "\n",
    "    # Annotate each point with its index\n",
    "    for j in range(number_of_sub_series):\n",
    "        plt.annotate(str(j), (latent_space_stock[j, 0], latent_space_stock[j, 1]))\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Latent Space Visualization with PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "UnLunNiuufGp",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "9bbea48c-025e-400f-fe29-5101d3eb7a51"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply PCA to the latent space\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# Set seeds for reproducibility\n",
    "\n",
    "print(signal_to_tensor.shape)\n",
    "with torch.no_grad():\n",
    "    signals_latent,_,_,_ = model.encoder(signal_to_tensor)\n",
    "    signals_latent = signals_latent.squeeze().cpu().detach().numpy()\n",
    "    print(signals_latent.shape)\n",
    "signals_latent_sumed = signals_latent.reshape(number_of_stocks,number_of_sub_series,latent_dim )\n",
    "\n",
    "signals_latent_sumed = np.sum(signals_latent_sumed, axis=(1))\n",
    "\n",
    "print(f\"signals_latent shape : {signals_latent_sumed.shape}\")\n",
    "pca = PCA(n_components=2)\n",
    "latent_space = pca.fit_transform(signals_latent_sumed)\n",
    "# Reshape the latent space for each stock\n",
    "print(latent_space.shape)\n",
    "latent_space_np_array = np.array(latent_space).reshape((number_of_stocks, 2))\n",
    "print(f\"latent_space_np_array shape : {latent_space_np_array.shape}\")\n",
    "# Plot the latent space for each stock\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, latent_space_stock in enumerate(latent_space_np_array):\n",
    "    plt.scatter(latent_space_stock[0], latent_space_stock[1], label=f'{stock_symbols[i]}')\n",
    "\n",
    "    # Annotate each point with its index\n",
    "    for j in range(number_of_sub_series):\n",
    "        plt.annotate(str(j), (latent_space_stock[0], latent_space_stock[1]))\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Latent Space Visualization with PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "ojyPyE1Kh_FA",
    "outputId": "a03e17d6-3c25-4554-bc4d-e586fc5746a2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply K-means to the latent space\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "K_means_PCA = KMeans(n_clusters=8, random_state=seed)\n",
    "K_means_PCA.fit(latent_space_np_array)\n",
    "inertia_Kmeans_PCA = K_means_PCA.inertia_\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(latent_space_np_array[:, 0], latent_space_np_array[:, 1], c=K_means_PCA.labels_, cmap='rainbow')\n",
    "plt.scatter(K_means_PCA.cluster_centers_[:, 0], K_means_PCA.cluster_centers_[:, 1], color='black')\n",
    "plt.title('K-means Clustering with PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " # Elbow Method for K means\n",
    "# Import ElbowVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "model_bis = KMeans()\n",
    "# k is range of number of clusters.\n",
    "visualizer = KElbowVisualizer(model_bis, k=(2,30), timings= True)\n",
    "# Fit data to visualizer\n",
    "visualizer.fit(latent_space_np_array)\n",
    "visualizer.show()        # Finalize and render figure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Silhouette Score for K means\n",
    "# Import ElbowVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "models = KMeans()\n",
    "# k is range of number of clusters.\n",
    "visualizer = KElbowVisualizer(models, k=(2,30),metric='silhouette', timings= True)\n",
    "visualizer.fit(latent_space_np_array)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cluster_to_plot = 0\n",
    "# Plot the signals in the cluster\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i, cluster in enumerate(K_means_PCA.labels_):\n",
    "    if cluster == cluster_to_plot:\n",
    "        plt.plot(stocks_np_array_reshaped[i].reshape((-1, 1)), label=f'{stock_symbols[i]}')\n",
    "plt.title(f'Stocks in Cluster {cluster_to_plot}')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# interpolate between two signales\n",
    "# take the first signal\n",
    "print(f\"latent_signal_np_array shape : {signals_latent.shape}\")\n",
    "signals_latent = signals_latent.reshape((number_of_stocks, number_of_sub_series, latent_dim))\n",
    "\n",
    "signal_1_index,signal_1_name = 79, stock_symbols[79]\n",
    "\n",
    "signal_2_index ,signal_2_name = 43, stock_symbols[43]\n",
    "signal_1 = signals_latent[signal_1_index]\n",
    "signal_2 = signals_latent[signal_2_index]\n",
    "\n",
    "# define the number of points to interpolate\n",
    "number_of_points = signal_1.shape[0]\n",
    "print(f\"number_of_points : {number_of_points}\")\n",
    "print(f\"signal_1 shape : {signal_1.shape}\")\n",
    "print(f\"signal_2 shape : {signal_2.shape}\")\n",
    "new_signal = np.zeros((number_of_points,latent_dim))\n",
    "\n",
    "for i in range(number_of_points):\n",
    "\n",
    "\n",
    "    new_signal[i] = (signal_1[i] + (signal_2[i]) ) / 2\n",
    "\n",
    "# apply PCA to the new signal\n",
    "#pca = PCA(n_components=2)\n",
    "#\n",
    "# plot the latent space with the 3 signals in different colors\n",
    "#latent_space_new_signal = pca.fit_transform(new_signal)\n",
    "latent_space_new_signal = new_signal\n",
    "plt.scatter(signal_1[:,0],signal_1[:,1],label=signal_1_name)\n",
    "plt.scatter(signal_2[:,0],signal_2[:,1],label=signal_2_name)\n",
    "plt.scatter(latent_space_new_signal[:,0],latent_space_new_signal[:,1],label='new signal')\n",
    "for i in range(number_of_points):\n",
    "    plt.annotate(str(i), (signal_1[i, 0], signal_1[i, 1]))\n",
    "    plt.annotate(str(i), (signal_2[i, 0], signal_2[i, 1]))\n",
    "    plt.annotate(str(i), (latent_space_new_signal[i, 0], latent_space_new_signal[i, 1]))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "tm5DahwSRz3n",
    "outputId": "588b72c5-0b39-4851-bb35-a4adfafdbb6d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# decode the new signal\n",
    "new_signal = torch.from_numpy(new_signal).float().to(device)\n",
    "new_signal = new_signal.squeeze()\n",
    "print(f\"new signal shape : {new_signal.shape}\")\n",
    "with torch.no_grad():\n",
    "    new_signal_decoded = model.decoder(new_signal)\n",
    "    new_signal_decoded = nn.Tanh()(new_signal_decoded)\n",
    "    new_signal_decoded = new_signal_decoded.squeeze().cpu().detach().numpy()\n",
    "print(f\"new signal decoded shape : {new_signal_decoded.shape}\")\n",
    "new_signal_decoded = new_signal_decoded.reshape((number_of_sub_series*sub_series_length,1))\n",
    "\n",
    "stocks_array_flatten = stocks_np_array.reshape((number_of_stocks,number_of_sub_series*sub_series_length,1))\n",
    "print(f\"stocks_array_flatten shape : {stocks_array_flatten.shape}\")\n",
    "# plot the new signal decoded, the first signal and the second signal\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.plot(stocks_array_flatten[signal_1_index],label=signal_1_name)\n",
    "plt.plot(stocks_array_flatten[signal_2_index],label=signal_2_name)\n",
    "plt.plot(new_signal_decoded,label='new signal decoded')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "dtobWaqXRz3n",
    "outputId": "5a97fe2c-f81e-4bef-f44d-0e4aebc40c0d"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TDhe8yubuc01"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
